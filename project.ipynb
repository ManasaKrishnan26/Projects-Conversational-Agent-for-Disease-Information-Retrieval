{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from Bio import Entrez\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________ORPHANET DATA_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of rare diseases are available here  -  https://www.orphadata.com/classifications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_from_folder(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_orphanet_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    diseases = []\n",
    "    for disorder in root.findall('.//Disorder'):\n",
    "        name = disorder.find('.//Name[@lang=\"en\"]')\n",
    "        orpha_code = disorder.find('.//OrphaCode')\n",
    "        if name is not None and orpha_code is not None:\n",
    "            diseases.append({'name': name.text, 'orpha_code': orpha_code.text})\n",
    "    return diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_dict = {}\n",
    "for file in read_files_from_folder('Rare_Diseases'):\n",
    "    if file.endswith('.xml'):\n",
    "        diseases = parse_orphanet_xml('Rare_Diseases/' + file)\n",
    "        file_name = file.split('.')[0]\n",
    "        diseases_dict[file_name] = diseases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------End Orphanet data------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(diseases_dict.items())[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------Begin Pubmed Data fetch------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = 'abc@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pubmed(query, max_results=10):\n",
    "    Entrez.email = EMAIL  # Always provide your email\n",
    "    query_with_filter = query + \" AND free full text[sb]\"  # Adding the free full text filter\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax=max_results,\n",
    "                            retmode='xml', \n",
    "                            term=query_with_filter)\n",
    "    results = Entrez.read(handle)\n",
    "\n",
    "    if results['IdList'] == []:\n",
    "        handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax=max_results,\n",
    "                            retmode='xml', \n",
    "                            term=query)\n",
    "        \n",
    "        results = Entrez.read(handle)\n",
    "    return results['IdList']\n",
    "\n",
    "\n",
    "def fetch_pubmed_details(id_list):\n",
    "    Entrez.email = EMAIL \n",
    "    if len(id_list) == 0:\n",
    "        return None\n",
    "    ids = ','.join(id_list)\n",
    "    handle = Entrez.efetch(db='pubmed', id=ids, retmode='xml')\n",
    "    papers = Entrez.read(handle)\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_articles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diseases = set()\n",
    "for disease_name, diseases in diseases_dict.items():\n",
    "    for disease in diseases:\n",
    "        all_diseases.add(disease['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disease in all_diseases:\n",
    "    if disease in disease_articles and disease_articles[disease] != []:\n",
    "        continue    \n",
    "    query = f'{disease}'\n",
    "    disease_articles[disease] = search_pubmed(query, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logged on 8th Dec - 12:09 AM - json dumped at 12:10 AM\n",
    "with open('disease_articles.json', 'w') as f:\n",
    "    json.dump(disease_articles, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file for disease articles\n",
    "with open('disease_articles.json', 'r') as f:\n",
    "    disease_articles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_list = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(disease_articles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for disease, ids in tqdm(disease_articles.items()):\n",
    "    if disease in papers_list and papers_list[disease] != []:\n",
    "        continue\n",
    "    papers_list[disease] = fetch_pubmed_details(ids)\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_articles_new = {key: value for key, value in disease_articles.items() if value}\n",
    "len(disease_articles_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_details(diseases_dict:dict, disease_articles:dict,combined_data:list):\n",
    "    for category, diseases in diseases_dict.items():\n",
    "        print(f\"Processing {category}...\")\n",
    "        if os.path.exists(f\"combined_data/{category}.json\"):\n",
    "            continue\n",
    "\n",
    "        for disease in tqdm(diseases):\n",
    "            disease_data = {\n",
    "                'name': disease['name'],\n",
    "                'orpha_code': disease['orpha_code'],\n",
    "                'articles': []\n",
    "            }\n",
    "            \n",
    "            if disease['name'] in disease_articles:\n",
    "                ids = disease_articles_new[disease['name']][:5]\n",
    "                papers = fetch_pubmed_details(ids)\n",
    "                time.sleep(0.5)\n",
    "                if papers:\n",
    "                    for paper in papers['PubmedArticle']:\n",
    "                        pmid = paper['MedlineCitation']['PMID'].title()\n",
    "                        # Extract necessary details from each paper\n",
    "                        article_data = paper['MedlineCitation']['Article']\n",
    "                        if article_data['ELocationID'] and article_data['ELocationID'][0].attributes['EIdType'] == 'doi':\n",
    "                            doi = article_data['ELocationID'][0].title()\n",
    "\n",
    "                        article_url = f\"https://doi.org/{doi}\" if doi else ''\n",
    "                        title = article_data['ArticleTitle']\n",
    "                        abstract = paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0] if 'Abstract' in paper['MedlineCitation']['Article'] else ''\n",
    "                        disease_data['articles'].append({'PMID':pmid, 'title': title, 'abstract': abstract, 'article_url': article_url})\n",
    "\n",
    "            combined_data.append(disease_data)\n",
    "\n",
    "\n",
    "        # Create the \"combined_data\" folder if it doesn't exist\n",
    "        if not os.path.exists(\"combined_data\"):\n",
    "            os.makedirs(\"combined_data\")\n",
    "\n",
    "        # Dump the combined_data dictionary to a JSON file in the \"combined_data\" folder\n",
    "        with open(f\"combined_data/{category}.json\", \"w\") as f:\n",
    "            json.dump(combined_data, f)\n",
    "\n",
    "        combined_data.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_article_details(diseases_dict, disease_articles_new,combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text_from_doi(doi_url):\n",
    "    # Use Selenium to handle JavaScript-enabled requests\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('window-size=1920x1080')  # Set the window size\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36')\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "    with webdriver.Chrome(options=options) as driver:\n",
    "        driver.get(doi_url)\n",
    "        time.sleep(2)\n",
    "        html = driver.page_source\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_tag = soup.find('article')\n",
    "    if article_tag:\n",
    "        article_text = article_tag.get_text()\n",
    "    else:\n",
    "        article_text = soup.get_text()\n",
    "\n",
    "    return article_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_tag = soup.find('article')\n",
    " \n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup(['script', 'style']):\n",
    "        script_or_style.extract()\n",
    "\n",
    "    # Get text\n",
    "    text = ''\n",
    "    if article_tag:\n",
    "        text = article_tag.get_text()\n",
    "    else:\n",
    "        text = soup.get_text()\n",
    "\n",
    "    # Break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "    # Drop blank lines and remove non-ascii characters\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('window-size=1920x1080')  # Set the window size\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36')\n",
    "options.add_argument('--headless')  # Run Chrome in headless mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_folder = 'combined_data'\n",
    "loaded_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in (os.listdir(combined_data_folder)):\n",
    "    print(f\"Processing {file_name}...\")\n",
    "    if file_name in loaded_files:\n",
    "        continue\n",
    "    file_path = os.path.join(combined_data_folder, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        final_data = json.load(f)\n",
    "        # Process the data here\n",
    "        with webdriver.Chrome(options=options) as driver:\n",
    "            for data in tqdm(final_data):\n",
    "                for article in data['articles']:\n",
    "                    if 'full_text' in article:\n",
    "                        continue\n",
    "                    if article['article_url']:\n",
    "                        driver.get(article['article_url'])\n",
    "                        time.sleep(1)\n",
    "                        html = driver.page_source\n",
    "                        # Parse the HTML using BeautifulSoup\n",
    "                        article_text = clean_html(html)\n",
    "                        article['full_text'] = article_text        # Add the loaded file to the list\n",
    "\n",
    "         \n",
    "        # Create the \"final_data\" folder if it doesn't exist\n",
    "        if not os.path.exists(\"final_data\"):\n",
    "            os.makedirs(\"final_data\")\n",
    "\n",
    "        # Dump the combined_data dictionary to a JSON file in the \"final_data\" folder\n",
    "        with open(f\"final_data/{file_name}_final.json\", \"w\") as f:\n",
    "            json.dump(final_data, f)  \n",
    "\n",
    "        loaded_files.append(file_name)   \n",
    "        final_data = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"final_data/Rare_systemic_and_rhumatological_diseases.json_final.json\", \"w\") as f:\n",
    "            json.dump(final_data, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_files.append(\"Rare_systemic_and_rhumatological_diseases.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
